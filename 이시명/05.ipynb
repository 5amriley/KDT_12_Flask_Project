{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.021844Z",
     "start_time": "2024-04-21T09:31:07.139942Z"
    }
   },
   "id": "cb959c895034d921",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Assuming 'data.csv' is already loaded into 'data' DataFrame\n",
    "data = pd.read_csv('data.csv', header=None)\n",
    "\n",
    "# Slice the DataFrame\n",
    "data_subset = data.iloc[:100]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.362295Z",
     "start_time": "2024-04-21T09:31:09.022438Z"
    }
   },
   "id": "c26cd2fbd6e84f0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 토큰화 및 정수 인코딩\n",
    "def tokenize(sentence):\n",
    "    return word_tokenize(sentence)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.366643Z",
     "start_time": "2024-04-21T09:31:09.362428Z"
    }
   },
   "id": "92ed29222a61f50b",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m word2idx \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<pad>\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m0\u001B[39m}\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m \u001B[43msentences\u001B[49m:\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokenize(sent[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m+\u001B[39m tokenize(sent[\u001B[38;5;241m1\u001B[39m]):\n\u001B[1;32m      4\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m word2idx:\n",
      "\u001B[0;31mNameError\u001B[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "word2idx = {\"<pad>\": 0}\n",
    "for sent in sentences:\n",
    "    for word in tokenize(sent[0]) + tokenize(sent[1]):\n",
    "        if word not in word2idx:\n",
    "            word2idx[word] = len(word2idx)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.604377Z",
     "start_time": "2024-04-21T09:31:09.365452Z"
    }
   },
   "id": "91e386c2cd3bdc0a",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def numericalize(sent):\n",
    "    return [word2idx[word] for word in tokenize(sent)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.602925Z"
    }
   },
   "id": "6d8f91690159c2b2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5e9fe87cfd0d8e9b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data = dataframe\n",
    "        self.src = self.data.iloc[:, 0]  # Assuming the source text is in the first column\n",
    "        self.trg = self.data.iloc[:, 1]  # Assuming the target text is in the second column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenize(self.src.iloc[idx])\n",
    "        trg = self.tokenize(self.trg.iloc[idx])\n",
    "        return torch.tensor(src, dtype=torch.long), torch.tensor(trg, dtype=torch.long)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [word.lower() for word in word_tokenize(text)]\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.604298Z"
    }
   },
   "id": "af2f8e36298f602a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = TranslationDataset(data_subset)\n",
    "\n",
    "train_size = int(0.7 * len(dataset))\n",
    "valid_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - valid_size\n",
    "train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, valid_size, test_size])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.608974Z",
     "start_time": "2024-04-21T09:31:09.605573Z"
    }
   },
   "id": "d01f661382b3b6e0",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# collate_fn 정의\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = [], []\n",
    "\n",
    "    for src, trg in batch:\n",
    "        src_batch.append(torch.tensor(src))\n",
    "        trg_batch.append(torch.tensor(trg))\n",
    "\n",
    "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=0, batch_first=True)\n",
    "    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=0, batch_first=True)\n",
    "\n",
    "    return src_batch, trg_batch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.606740Z"
    }
   },
   "id": "d664bc1969cd30bc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.607869Z"
    }
   },
   "id": "9ac164d8418c74df",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "  def __init__(self, input_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = torch.nn.Embedding(input_size, hidden_size)\n",
    "    self.rnn = torch.nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "  def forward(self, input):\n",
    "    embedded = self.embedding(input)\n",
    "    output, hidden = self.rnn(embedded)\n",
    "    return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-21T09:31:09.609455Z",
     "start_time": "2024-04-21T09:31:09.609040Z"
    }
   },
   "id": "e0063bd997ca09ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "  def __init__(self, output_size, hidden_size):\n",
    "    super().__init__()\n",
    "    self.embedding = torch.nn.Embedding(output_size, hidden_size)\n",
    "    self.rnn = torch.nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "    self.out = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "  def forward(self, input, hidden, encoder_output):\n",
    "    embedded = self.embedding(input)\n",
    "    output, hidden = self.rnn(embedded, hidden)\n",
    "    output = self.out(output)\n",
    "    return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.610288Z"
    }
   },
   "id": "1c1bc067a556d565",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Seq2Seq(torch.nn.Module):\n",
    "  def __init__(self, encoder, decoder):\n",
    "    super().__init__()\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "\n",
    "  def forward(self, src, trg):\n",
    "    encoder_output, encoder_hidden = self.encoder(src)\n",
    "    decoder_output, decoder_hidden = self.decoder(trg, encoder_hidden, encoder_output)\n",
    "    return decoder_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.611354Z"
    }
   },
   "id": "b85d88419cfea3a4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "input_size = len(word2idx)\n",
    "hidden_size = 256\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "decoder = Decoder(input_size, hidden_size)\n",
    "model = Seq2Seq(encoder, decoder)\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.612444Z"
    }
   },
   "id": "92d312ea892603f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, trg in loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg[:, :-1])  # trg input does not include the <eos> token\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)  # trg shifted for loss calculation, does not include <sos>\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.613190Z"
    }
   },
   "id": "fba15251a4bc2eb5",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.613989Z"
    }
   },
   "outputs": [],
   "source": [
    "# 평가 함수\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in loader:\n",
    "            output = model(src, trg)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 학습 과정\n",
    "n_epochs = 1\n",
    "clip = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, clip)\n",
    "    valid_loss = evaluate(model, valid_loader, criterion)\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.3f}, Valid Loss: {valid_loss:.3f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.614817Z"
    }
   },
   "id": "89f8a372c2019679",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# 최적 모델 로드 및 테스트\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.615568Z"
    }
   },
   "id": "3f01c9597769db6",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.616352Z"
    }
   },
   "id": "19184c4b2cdd6465",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.617074Z"
    }
   },
   "id": "62dd3b621708a642"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-04-21T09:31:09.618123Z"
    }
   },
   "id": "7eba3cbbbd03faa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
