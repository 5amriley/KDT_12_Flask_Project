{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b708ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import torch \n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832d496c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence 1: 우리 컨텐츠가 또 나왔으면 좋겠어요!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"../model/model3.pth\", map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "prompt_text = \"우리\"\n",
    "# 텍스트를 토큰 ID의 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")  # 입력 데이터를 모델과 동일한 장치로 이동\n",
    "# 텍스트 생성\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=100,  # 생성할 최대 길이 설정\n",
    "    num_return_sequences=1,  # 생성할 텍스트 수 설정\n",
    "    temperature=0.7,  # 샘플링 온도 설정 (낮을수록 보수적인 결과, 높을수록 더 다양한 결과)\n",
    "    top_k=50,  # 상위 k개의 토큰 중에서만 샘플링\n",
    "    top_p=0.9,  # 누적 확률이 이 값을 넘지 않도록 하여 상위 p%의 토큰만 사용\n",
    "    pad_token_id=tokenizer.pad_token_id,  # 패딩 토큰 ID\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 문장 종료 토큰 ID\n",
    "    bos_token_id=tokenizer.bos_token_id,  # 문장 시작 토큰 ID\n",
    "    num_beams=2,  # 빔 서치 사용 (다양한 결과를 얻기 위해 빔 서치를 사용할 수도 있음)\n",
    "    early_stopping=True,  # 조건에 만족하는 첫 번째 생성 결과 반환\n",
    "    do_sample=True,\n",
    ")\n",
    "# 생성된 텍스트 출력\n",
    "for i, sequence in enumerate(output):\n",
    "    print(f\"Generated sequence {i+1}: {tokenizer.decode(sequence, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a296934d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated sequence 1: 모드 퀄리티 쩔어요 ᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"../model/model3.pth\", map_location=torch.device('cpu'))\n",
    "model.eval()\n",
    "prompt_text = \"모드\"\n",
    "# 텍스트를 토큰 ID의 시퀀스로 변환\n",
    "input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")  # 입력 데이터를 모델과 동일한 장치로 이동\n",
    "# 텍스트 생성\n",
    "output = model.generate(\n",
    "    input_ids=input_ids,\n",
    "    max_length=100,  # 생성할 최대 길이 설정\n",
    "    num_return_sequences=1,  # 생성할 텍스트 수 설정\n",
    "    temperature=0.7,  # 샘플링 온도 설정 (낮을수록 보수적인 결과, 높을수록 더 다양한 결과)\n",
    "    top_k=50,  # 상위 k개의 토큰 중에서만 샘플링\n",
    "    top_p=0.9,  # 누적 확률이 이 값을 넘지 않도록 하여 상위 p%의 토큰만 사용\n",
    "    pad_token_id=tokenizer.pad_token_id,  # 패딩 토큰 ID\n",
    "    eos_token_id=tokenizer.eos_token_id,  # 문장 종료 토큰 ID\n",
    "    bos_token_id=tokenizer.bos_token_id,  # 문장 시작 토큰 ID\n",
    "    num_beams=2,  # 빔 서치 사용 (다양한 결과를 얻기 위해 빔 서치를 사용할 수도 있음)\n",
    "    early_stopping=True,  # 조건에 만족하는 첫 번째 생성 결과 반환\n",
    "    do_sample=True,\n",
    ")\n",
    "# 생성된 텍스트 출력\n",
    "for i, sequence in enumerate(output):\n",
    "    print(f\"Generated sequence {i+1}: {tokenizer.decode(sequence, skip_special_tokens=True)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5224c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('하신다', 0.7950177192687988), ('실력', 0.7754234671592712), ('악역', 0.7479884624481201), ('배우', 0.7398842573165894), ('캐릭터', 0.7372451424598694)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec.load(\"../model/Gensim.model\")\n",
    "word = \"연기\"\n",
    "print(word2vec.wv.most_similar(word, topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f4969d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text): # 댓글 생성 함수 만들기\n",
    "    tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\", bos_token='</s>', eos_token='</s>', unk_token='<unk>', pad_token='<pad>', mask_token='<mask>')\n",
    "    model = GPT2LMHeadModel.from_pretrained(\"skt/kogpt2-base-v2\")\n",
    "    model = torch.load(\"../model/model3.pth\", map_location=torch.device('cpu'))\n",
    "    model.eval()\n",
    "    prompt_text = text\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors=\"pt\")\n",
    "    output = model.generate(input_ids=input_ids,max_length=100,num_return_sequences=1,temperature=0.7, top_k=50, pad_token_id=tokenizer.pad_token_id,eos_token_id=tokenizer.eos_token_id,bos_token_id=tokenizer.bos_token_id,num_beams=2,early_stopping=True,do_sample=True,\n",
    "    )\n",
    "    for i, sequence in enumerate(output):\n",
    "        result = tokenizer.decode(sequence, skip_special_tokens=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ab9e749",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "word_list = word2vec.wv.most_similar(\"마크\", topn=5)\n",
    "comment_list=[]\n",
    "for i in word_list:\n",
    "    comment_list.append(generate_text(i[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fe09cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['존경할게요!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '합니다!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " '평생 웃기네요 ᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏᄏ',\n",
       " '',\n",
       " '해여\\n\"<a href=\"\"https://www.youtube.com/watch?v=5aW7vYdMUk0&amp;t=1764\"\">29:24</a> <a href=\"\"https://www.youtube.com/watch?v=5aW7vYdMUk']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bffd9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
